# Definition
Big O is a mathematical notation used to the define the time and memory requirements of an algorithm. It helps software engineers make decisions on which data structures or algorithms to use when solving a problem. 

	As input grows, how fast does the time taken or memory used grow?

Note that the growth is always with respect to input.

# What To Note When Calculating Big O
1. Checks for loops in the algorithm and how many times elements are iterated over.
2. Drop constants.